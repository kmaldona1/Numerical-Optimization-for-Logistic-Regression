# Optimization Algorithms Comparison

This assignment compares the performance of four optimization algorithms: Gradient Descent (GD), Stochastic Gradient Descent (SGD), and Mini-Batch Stochastic Gradient Descent (MB-SGD) with batch sizes 8 and 64. The comparison is based on their ability to minimize a strongly convex objective function for logistic regression.

## Overview

Gradient-based optimization algorithms are essential for training machine learning models. This assignment aims to illustrate the convergence behaviors and efficiency of different gradient-based methods. The algorithms compared are:
- Gradient Descent (GD)
- Stochastic Gradient Descent (SGD)
- Mini-Batch Stochastic Gradient Descent (MB-SGD) with batch size 8
- Mini-Batch Stochastic Gradient Descent (MB-SGD) with batch size 64

## Files
Please refer to Programming1.html for main script containing the output of each cell and generated plot. The file also includes plain-english comparison between the four (4) algorithms. 
